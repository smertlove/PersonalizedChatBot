{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43fe1562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c34bd2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import ChatBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ede551b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-26 00:33:36] INFO SentenceTransformer.py:211: Use pytorch device_name: cuda:0\n",
      "[2026-01-26 00:33:36] INFO SentenceTransformer.py:219: Load pretrained SentenceTransformer: sch-allie/bert-persona-model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init cold start...\n",
      "Init extractor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-26 00:33:46] INFO SentenceTransformer.py:211: Use pytorch device_name: cuda:0\n",
      "[2026-01-26 00:33:46] INFO SentenceTransformer.py:219: Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init db retriever...\n",
      "Init generator...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready!!\n"
     ]
    }
   ],
   "source": [
    "chatbot = ChatBot(use_vllm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f19f8a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 224.78it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.82 s, sys: 122 ms, total: 4.94 s\n",
      "Wall time: 4.95 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'It’s definitely possible you’re an anime fan! Many people who enjoy video games and jazz also have an interest in anime, and your love for music and karate suggests a creative side – it’s a really interesting combination.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "chatbot.response(\"\"\"\n",
    "I am 32. I do not want a job. I play video games all day. I still live at home with my parents. My favorite drink is iced coffee. I have a black belt in karate. I m in a jazz band and play the saxophone. I vacation along lake michigan every summer.\n",
    "\"\"\")\n",
    "chatbot.response(\"I really want to play Genshin Impact all night, but I'm afraid my parents will punish me.\")\n",
    "chatbot.response(\"Am I an anime fan?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85476165",
   "metadata": {},
   "source": [
    "### Ручная перезагрузка тетрадки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cee08d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-26 00:34:41] INFO SentenceTransformer.py:211: Use pytorch device_name: cuda:0\n",
      "[2026-01-26 00:34:41] INFO SentenceTransformer.py:219: Load pretrained SentenceTransformer: sch-allie/bert-persona-model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init cold start...\n",
      "Init extractor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-26 00:34:51] INFO SentenceTransformer.py:211: Use pytorch device_name: cuda:0\n",
      "[2026-01-26 00:34:51] INFO SentenceTransformer.py:219: Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init db retriever...\n",
      "Init generator...\n",
      "INFO 01-26 00:34:55 [utils.py:263] non-default args: {'tokenizer': 'google/gemma-3-1b-it', 'trust_remote_code': True, 'dtype': torch.bfloat16, 'max_model_len': 2048, 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': False, 'model': 'google/gemma-3-1b-it'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-26 00:34:56 [model.py:530] Resolved architecture: Gemma3ForCausalLM\n",
      "INFO 01-26 00:34:56 [model.py:1545] Using max model len 2048\n",
      "WARNING 01-26 00:34:56 [arg_utils.py:1918] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-26 00:34:56,270\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-26 00:34:56 [vllm.py:630] Asynchronous scheduling is enabled.\n",
      "INFO 01-26 00:34:56 [vllm.py:637] Disabling NCCL for DP synchronization when using async scheduling.\n",
      "WARNING 01-26 00:34:58 [system_utils.py:136] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n",
      "\u001b[0;36m(EngineCore_DP0 pid=6088)\u001b[0;0m INFO 01-26 00:35:02 [core.py:97] Initializing a V1 LLM engine (v0.14.1) with config: model='google/gemma-3-1b-it', speculative_config=None, tokenizer='google/gemma-3-1b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=google/gemma-3-1b-it, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=6088)\u001b[0;0m INFO 01-26 00:35:03 [parallel_state.py:1214] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.1.14:53995 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=6088)\u001b[0;0m INFO 01-26 00:35:03 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A\n",
      "\u001b[0;36m(EngineCore_DP0 pid=6088)\u001b[0;0m INFO 01-26 00:35:04 [gpu_model_runner.py:3808] Starting to load model google/gemma-3-1b-it...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=6088)\u001b[0;0m INFO 01-26 00:35:04 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n",
      "\u001b[0;36m(EngineCore_DP0 pid=6088)\u001b[0;0m INFO 01-26 00:35:05 [weight_utils.py:550] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.00it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.00it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=6088)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=6088)\u001b[0;0m INFO 01-26 00:35:06 [default_loader.py:291] Loading weights took 0.28 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=6088)\u001b[0;0m INFO 01-26 00:35:06 [gpu_model_runner.py:3905] Model loading took 1.91 GiB memory and 1.577287 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=6088)\u001b[0;0m INFO 01-26 00:35:11 [backends.py:644] Using cache directory: /home/smertlove/.cache/vllm/torch_compile_cache/47009b91c4/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=6088)\u001b[0;0m INFO 01-26 00:35:11 [backends.py:704] Dynamo bytecode transform time: 4.54 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=6088)\u001b[0;0m INFO 01-26 00:35:14 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 0.938 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=6088)\u001b[0;0m INFO 01-26 00:35:14 [monitor.py:34] torch.compile takes 5.48 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=6088)\u001b[0;0m INFO 01-26 00:35:15 [gpu_worker.py:358] Available KV cache memory: 1.78 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=6088)\u001b[0;0m WARNING 01-26 00:35:15 [kv_cache_utils.py:1047] Add 2 padding layers, may waste at most 9.09% KV cache memory\n",
      "\u001b[0;36m(EngineCore_DP0 pid=6088)\u001b[0;0m INFO 01-26 00:35:15 [kv_cache_utils.py:1305] GPU KV cache size: 66,480 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=6088)\u001b[0;0m INFO 01-26 00:35:15 [kv_cache_utils.py:1310] Maximum concurrency for 2,048 tokens per request: 32.25x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:01<00:00, 29.90it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:00<00:00, 38.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=6088)\u001b[0;0m INFO 01-26 00:35:18 [gpu_model_runner.py:4856] Graph capturing finished in 3 secs, took 0.44 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=6088)\u001b[0;0m INFO 01-26 00:35:18 [core.py:273] init engine (profile, create kv cache, warmup model) took 12.02 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=6088)\u001b[0;0m INFO 01-26 00:35:20 [vllm.py:630] Asynchronous scheduling is enabled.\n",
      "INFO 01-26 00:35:21 [llm.py:347] Supported tasks: ['generate']\n",
      "Ready!!\n"
     ]
    }
   ],
   "source": [
    "chatbot = ChatBot(use_vllm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6756af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 474 ms, sys: 11.9 ms, total: 486 ms\n",
      "Wall time: 2.32 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'As a jazz musician, you certainly have a creative and expressive side – it’s interesting to think about how that might intersect with your interest in anime!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "chatbot.response(\"\"\"\n",
    "I am 32. I do not want a job. I play video games all day. I still live at home with my parents. My favorite drink is iced coffee. I have a black belt in karate. I m in a jazz band and play the saxophone. I vacation along lake michigan every summer.\n",
    "\"\"\")\n",
    "chatbot.response(\"I really want to play Genshin Impact all night, but I'm afraid my parents will punish me.\")\n",
    "chatbot.response(\"Am I an anime fan?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e498e02f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
